# Neural language notes
Simple notes on papers about neural language learning from arxiv, ACL, EMNLP, NAACL, and some machine/deep learning from ICLR, ICML, NIPS. This note is inspired by [Denny Britz's notes](https://github.com/dennybritz/deeplearning-papernotes). There is also dataset for neural language research [[link](https://github.com/dykang/neurallanguage-data)].

#### TODO
- [ ] votes for papers (e.g., :+1:)
- [ ] automatic crawler for citation and search counts (e.g., cite+51, tweets+42, search+523 )

#### Accepted papers
 - [[NAACL16](http://naacl.org/naacl-hlt-2016/accepted_papers.html)], [[ACL16](http://acl2016.org/index.php?article_id=68)], [[EMNLP16](http://www.emnlp2016.net/accepted-papers.html)], [[NIPS16](https://nips.cc/Conferences/2016/AcceptedPapers)], [[ICML16](http://icml.cc/2016/?page_id=1649)], [[ICLR16](http://www.iclr.cc/doku.php?id=iclr2016:main&#accepted_papers_conference_track)] [[DLSC16](https://sites.google.com/site/deeplearningsummerschool2016/speakers), [note](https://github.com/dykang/neurallanguage-notes/blob/master/notes/DLSC16.md)]

#### Groups
 - [[DeepMind](https://deepmind.com/publications.html)], [[GoogleBrain](http://research.google.com/pubs/BrainTeam.html)], [[FAIR](https://research.facebook.com/publications/ai/)] [[AI2](http://allenai.org/papers.html)], [[MSR](https://www.microsoft.com/en-us/research/research-area/natural-language-processing-speech/?q&content-type=publications)]
 - [[CMU](https://www.lti.cs.cmu.edu/work)] [[Stanford](http://nlp.stanford.edu/pubs/)]  [[Berkeley](http://nlp.cs.berkeley.edu/publications.shtml)] [[Montreal](https://mila.umontreal.ca/en/publications/)] [[UW](https://www.cs.washington.edu/research/nlp/publications-by-year)]


#### 2016-09

#### 2016-08
- Deep Learning without Poor Local Minima [[nips](https://arxiv.org/abs/1605.07110)]
- Actor-critic versus direct policy search: a comparison based on sample complexity [[arxiv](https://arxiv.org/abs/1606.09152)]
- Policy Networks with Two-Stage Training for Dialogue Systems [[arxiv](https://arxiv.org/abs/1606.03152)]
- Pointing the Unknown Words [[acl](https://arxiv.org/abs/1603.08148)]
- An Incremental Parser for Abstract Meaning Representation [[arxiv](https://arxiv.org/abs/1608.06111)]
- Topic Sensitive Neural Headline Generation [[arxiv](https://arxiv.org/abs/1608.05777)]
- Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine [[interspeech](https://arxiv.org/abs/1608.06378)]
- Face2Face: Real-time Face Capture and Reenactment of RGB Videos [[demo](https://www.youtube.com/watch?v=ohmajJTcpNk)]
- Image-Space Modal Bases for Plausible Manipulation of Objects in Video [[demo](http://www.interactivedynamicvideo.com/publications.html)]
- Decoupled neural interfaces using synthetic gradients [[arxiv](http://arxiv.org/abs/1608.05343) [blog](http://deliprao.com/archives/187)]
- Full Resolution Image Compression with Recurrent Neural Networks [[arxiv](http://arxiv.org/abs/1608.05148)]
- Who did What: A Large-Scale Person-Centered Cloze Dataset [[arxiv](https://arxiv.org/pdf/1608.05457v1.pdf) [data](https://tticnlp.github.io/who_did_what/)]
- Pixel Recurrent Neural Networks [[arxiv](https://arxiv.org/pdf/1601.06759v3.pdf)]
- Mollifying Networks [[arxiv](https://arxiv.org/abs/1608.04980)]
- Variational Information Maximizing Exploration [[arxiv](https://arxiv.org/abs/1605.09674)]
- Does Multimodality Help Human and Machine for Translation and Image Captioning [[arxiv](https://arxiv.org/abs/1605.09186)]
- Learning values across many orders of magnitude [[arxiv](https://arxiv.org/abs/1602.07714)]
- Attend, Infer, Repeat: Fast Scene Understanding with Generative Models [[arxiv](Attend, Infer, Repeat: Fast Scene Understanding with Generative Models)]
- Architectural Complexity Measures of Recurrent Neural Network [[arxiv](http://arxiv.org/pdf/1602.08210v2.pdf)]
- Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge [[arxiv](https://arxiv.org/pdf/1608.03000v1.pdf)]
- Canonical Correlation Inference for Mapping Abstract Scenes to Text [[arxiv](https://arxiv.org/pdf/1608.02784v1.pdf)]
- Temporal Attention Model for Neural Machine Translation [[arxiv](https://arxiv.org/pdf/1608.02927v1.pdf)]
- Bi-directional Attention with Agreement for Dependency Parsing [[arxiv](https://arxiv.org/pdf/1608.02076v1.pdf)]
- Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change [[arxiv](https://arxiv.org/pdf/1605.09096v3.pdf)]
- Recurrent Highway Networks [[arxiv](https://arxiv.org/abs/1607.03474)]
- Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond [[arxiv](https://arxiv.org/pdf/1602.06023v4.pdf)]
- WIKIREADING: A Novel Large-scale Language Understanding Task over Wikipedia [[arxiv](https://arxiv.org/abs/1608.03542)]
- Larger-Context Language Modelling with Recurrent Neural Network [[acl16](http://www.aclweb.org/anthology/P/P16/P16-1125.pdf)]
- Learning Online Alignments with Continuous Rewards Policy Gradient [[arxiv](http://arxiv.org/pdf/1608.01281.pdf)]
- Issues in evaluating semantic spaces using word analogies [[acl16](http://aclweb.org/anthology/W/W16/W16-2503.pdf)]
- Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks [[arxiv](http://arxiv.org/pdf/1605.09674v1.pdf)]
- Control of Memory, Active Perception, and Action in Minecraft [[icml16](http://jmlr.org/proceedings/papers/v48/oh16.pdf)]
- Dueling Network Architectures for Deep Reinforcement Learning [[arxiv](http://arxiv.org/pdf/1511.06581v3.pdf)]
- Human-level control through deep reinforcement learning [[nature](https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf)]
- Reinforcement Learning in Multi-Party Trading Dialog [[arxiv](http://www.sigdial.org/workshops/conference16/proceedings/pdf/SIGDIAL5.pdf)]
- Large-scale Simple Question Answering with Memory Network [[arxiv](http://arxiv.org/pdf/1506.02075v1.pdf)]
- On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems [[arxiv](https://arxiv.org/pdf/1605.07669v2.pdf)]
- A New Method to Visualize Deep Neural Networks [[arxiv](http://arxiv.org/pdf/1603.02518v2.pdf)]
- Dreaming of names with RBMs [[blog](https://colinmorris.github.io/blog/dreaming-rbms)]
- Synthesizing Compound Words for Machine Translation [[acl16](http://www.aclweb.org/anthology/P/P16/P16-1103.pdf)]
- Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations [[arxiv](http://arxiv.org/pdf/1606.01305v2.pdf)]
- Learning to Transduce with Unbounded Memory [[arxiv](http://arxiv.org/pdf/1506.02516v3.pdf)]
- Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets [[nip16](http://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.pdf)]
- LEARNING LONGER MEMORY IN RECURRENT NEURAL NETWORKS [[iclr15](http://arxiv.org/pdf/1412.7753v2.pdf)]
- Attention-based Multimodal Neural Machine Translation [[acl16](http://www.aclweb.org/anthology/W/W16/W16-2360.pdf)]
- A Two-stage Approach for Extending Event Detection to New Types via Neural Networks [[acl16](http://www.aclweb.org/anthology/W/W16/W16-16.pdf#page=172)]
- Learning text representation using recurrent convolutional neural network with highway layers [[arxiv](https://arxiv.org/abs/1606.06905)]
- Training Very Deep Networks [[arxiv](http://arxiv.org/abs/1507.06228)]
- SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity [[arxiv](https://arxiv.org/pdf/1608.00869v2.pdf)]
- Counter-fitting Word Vectors to Linguistic Constraints [[naacl16](http://mi.eng.cam.ac.uk/~nm480/naaclhlt2016.pdf)]
- Learning Online Alignments with Continuous Rewards Policy Gradient [[arxiv](https://arxiv.org/abs/1608.01281)]
- NEURAL PROGRAMMER: INDUCING LATENT PROGRAMS WITH GRADIENT DESCENT [[iclr16](https://arxiv.org/abs/1511.04834)]
- Supervised Attentions for Neural Machine Translation [[arxiv](https://arxiv.org/pdf/1608.00112v1.pdf)]
- A Neural Knowledge Language Model [[arxiv](https://arxiv.org/pdf/1608.00318v1.pdf)]
- Recurrent Models of Visual Attention [[cvpr](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf)]
- XGBoost: A Scalable Tree Boosting System [[arxiv](http://arxiv.org/pdf/1603.02754v3.pdf)]
- A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories [[naacl](http://aclweb.org/anthology/N/N16/N16-1098.pdf)]
- Deep Learning Trends @ ICLR 2016 [[blog](http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html)]
- Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift [[arxiv](http://arxiv.org/pdf/1502.03167v3.pdf)]
- VISUALIZING AND UNDERSTANDING RECURRENT NETWORKS [[iclr](https://arxiv.org/pdf/1506.02078v2.pdf)]
- Net2Net: ACCELERATING LEARNING VIA KNOWLEDGE TRANSFER [[iclr](http://arxiv.org/pdf/1511.05641v4.pdf)]
- A Latent Variable Recurrent Neural Network for Discourse Relation Language Models [[arxiv](http://arxiv.org/pdf/1603.01913v2.pdf)]
- A Recurrent Latent Variable Model for Sequential Data [[arxiv](https://arxiv.org/pdf/1506.02216.pdf)]
- ORDER-EMBEDDINGS OF IMAGES AND LANGUAGE [[iclr](http://arxiv.org/pdf/1511.06361v6.pdf)]
- Neural Module Networks [[arxiv](https://arxiv.org/pdf/1511.02799v3.pdf)]
- Learning to Compose Neural Networks for Question Answering [[acl](https://aclweb.org/anthology/N/N16/N16-1181.pdf)]
- Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units [[arxiv](https://arxiv.org/pdf/1603.05201v2.pdf)]

#### 2016-07
- ADVERSARIAL EXAMPLES IN THE PHYSICAL WORLD [[arxiv](https://arxiv.org/abs/1607.02533)]
- An Actor-Critic Algorithm for Sequence Prediction [[arxiv](http://arxiv.org/abs/1607.07086)]
- Enriching Word Vectors with Subword Information [[arxiv](http://arxiv.org/pdf/1607.04606.pdf)]
 - each word is represented as a bag of character n-grams in skip-gram
- Neural Machine Translation with Recurrent Attention Modeling [[arxiv](http://arxiv.org/pdf/1607.05108.pdf)]
- The Role of Discourse Units in Near-Extractive Summarization [[arxiv](http://www.cs.columbia.edu/~kapil/documents/sigdial16edus.pdf)]
- Bag of Tricks for Efficient Text Classification [[arxiv](http://arxiv.org/pdf/1607.01759.pdf)]
- Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks [[arxiv](http://arxiv.org/pdf/1607.01426.pdf)]
- Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes [[arxiv](http://arxiv.org/pdf/1607.00036.pdf)]
- STransE: a novel embedding model of entities and relationships in knowledge bases [[naacl16](https://arxiv.org/pdf/1606.08140v2.pdf)]
- Layer Normalization [[arxiv](https://arxiv.org/abs/1607.06450)]
- Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering [[arxiv](https://arxiv.org/pdf/1607.06275v1.pdf)]
- Imitation Learning with Recurrent Neural Networks [[arxiv](https://arxiv.org/pdf/1607.05241v1.pdf)]
- Neural Name Translation Improves Neural Machine Translation [[arxiv](https://arxiv.org/abs/1607.01856)]
- query-regression networks for machine comprehension [[arxiv](https://arxiv.org/abs/1606.04582)]
- Bag of Tricks for Efficient Text Classification [[arxiv](https://arxiv.org/abs/1607.01759)]
- Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks [[arxiv](https://arxiv.org/abs/1607.01426)]
- Sort Story: Sorting Jumbled Images and Captions into Stories [[arxiv](https://arxiv.org/pdf/1606.07493v3.pdf)]
- Separating Answers from Queries for Neural Reading Comprehension [[arxiv](https://arxiv.org/pdf/1607.03316v2.pdf)]
- Recurrent Highway Networks [[arxiv](https://arxiv.org/abs/1607.03474)]
- Charagram: Embedding Words and Sentences via Character n-grams [[arxiv](https://arxiv.org/abs/1607.02789)]
- ADVERSARIAL EXAMPLES IN THE PHYSICAL WORLD [[arxiv](https://arxiv.org/pdf/1607.02533v1.pdf)]
- Syntax-based Attention Model for Natural Language Inference [[arxiv](https://arxiv.org/pdf/1607.06556v1.pdf)]
- Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge [[arxiv](https://arxiv.org/abs/1607.03542)]
- Layer Normalization [[arxiv](http://arxiv.org/pdf/1607.06450.pdf)]
- Neural Sentence Ordering [[arxiv](https://arxiv.org/pdf/1607.06952v1.pdf) [code](https://github.com/FudanNLP/NeuralSentenceOrdering)]
- Distilling Word Embeddings: An Encoding Approach [[arxiv](https://arxiv.org/pdf/1506.04488v2.pdf)]
- Target-Side Context for Discriminative Models in Statistical Machine Translation [[arxiv](https://arxiv.org/pdf/1607.01149v1.pdf)]
- Domain Adaptation for Neural Networks by Parameter Augmentation [[arxiv](https://arxiv.org/pdf/1607.00410v1.pdf)]
- Towards Abstraction from Extraction: Multiple Timescale Gated Recurrent Unit for Summarization [[arxiv](https://arxiv.org/pdf/1607.00718v1.pdf)]
- Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes [[arxiv](https://arxiv.org/pdf/1607.00036v1.pdf)]
- Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering [[arxiv](https://arxiv.org/pdf/1607.06275v1.pdf)]
- Imitation Learning with Recurrent Neural Networks [[arxiv](https://arxiv.org/pdf/1607.05241v1.pdf)]
- Attention-over-Attention Neural Networks for Reading Comprehension [[arxiv](https://arxiv.org/pdf/1607.04423v2.pdf)]
- Neural Tree Indexers for Text Understanding [[arxiv](https://arxiv.org/pdf/1607.04492v1.pdf)]
- Generating Images Part by Part with Composite Generative Adversarial Networks [[arxiv](https://arxiv.org/pdf/1607.05387v1.pdf)]



#### 2016-06
- Neural Summarization by Extracting Sentences and Words [[arxiv](https://arxiv.org/pdf/1603.07252v3.pdf)]
- Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks [[arxiv](http://arxiv.org/pdf/1606.07461.pdf)]
- Sequence-Level Knowledge Distillation [[arxiv](http://arxiv.org/pdf/1606.07947.pdf)]
- Text Understanding with the Attention Sum Reader Network [[arxiv](https://arxiv.org/pdf/1603.01547v2.pdf)]
- Query-Regression Networks for Machine Comprehension [[arxiv](https://arxiv.org/pdf/1606.04582v1.pdf)]
- A Correlational Encoder Decoder Architecture for Pivot Based Sequence Generation [[arxiv](https://arxiv.org/pdf/1606.04754v1.pdf)]
- Smart Reply: Automated Response Suggestion for Email [[arxiv](https://arxiv.org/pdf/1606.04870v1.pdf)]
- Minimum Risk Training for Neural Machine Translation [[arxiv](https://arxiv.org/pdf/1512.02433v3.pdf)]
- Compression of Neural Machine Translation Models via Pruning [[arxiv](https://arxiv.org/abs/1606.09274)]
- Sort Story: Sorting Jumbled Images and Captions into Stories [[arxiv](https://arxiv.org/abs/1606.07493)]
- Dialog state tracking, a machine reading approach using a memory-enhanced neural network [[arxiv](https://arxiv.org/abs/1606.04052)]
- Predicting the Relative Difficulty of Single Sentences With and Without Surrounding Context [[arxiv](https://arxiv.org/abs/1606.08425)]
- Learning Generative ConvNet with Continuous Latent Factors by Alternating Back-Propagation [[arxiv](https://arxiv.org/pdf/1606.08571v1.pdf)]
- Topic Augmented Neural Response Generation with a Joint Attention Mechanism [[arxiv](https://arxiv.org/pdf/1606.08340v1.pdf)]
- STransE: a novel embedding model of entities and relationships in knowledge bases [[arxiv](https://arxiv.org/pdf/1606.08140v1.pdf)]
- Functional Distributional Semantics [[arxiv](https://arxiv.org/pdf/1606.08003v1.pdf)]
- Sequence-Level Knowledge Distillation [[arxiv](https://arxiv.org/pdf/1606.07947v1.pdf)]
- The LAMBADA dataset: Word prediction requiring a broad discourse context [[arxiv](http://arxiv.org/pdf/1606.06031v1.pdf)]
- DenseCap: Fully Convolutional Localization Networks for Dense Captioning [[link](http://cs.stanford.edu/people/karpathy/densecap.pdf)]
- Visualizing Dynamics: from t-SNE to SEMI-MDPs [[arxiv](https://arxiv.org/pdf/1606.07112v1.pdf)]
- Algorithmic Composition of Melodies with Deep Recurrent Neural Networks [[arxiv](https://arxiv.org/pdf/1606.07251v1.pdf)]
- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets [[arxiv](http://arxiv.org/pdf/1606.03657.pdf)]
- Deep Reinforcement Learning for Dialogue Generation [[arxiv](http://arxiv.org/pdf/1606.01541.pdf)]
- Key-Value Memory Networks for Directly Reading Documents [[arxiv]()]
- A Correlational Encoder Decoder Architecture for Pivot Based Sequence Generation [[arxiv](http://arxiv.org/pdf/1606.04754.pdf)]
- The Word Entropy of Natural Languages [[arxiv](https://arxiv.org/pdf/1606.06996v1.pdf)]
- Semantic Parsing to Probabilistic Programs for Situated Question Answering [[arxiv](https://arxiv.org/pdf/1606.07046v1.pdf)]
- Critical Behavior from Deep Dynamics: A Hidden Dimension in Natural Language [[arxiv](https://arxiv.org/pdf/1606.06737v1.pdf)]
- Inferring Logical Forms From Denotations [[arxiv](https://arxiv.org/pdf/1606.06900v1.pdf)]
- some notes from NAACL'16 Deep Learning panel discussion
 - Jacob Eisenstein made an observation, "In NLP, the things we do well on are things where context doesn't matter."
- Rationalizing Neural Predictions [[arxiv](https://arxiv.org/pdf/1606.04155v1.pdf)]
- DeepMath - Deep Sequence Models for Premise Selection [[arxiv](https://arxiv.org/pdf/1606.04442v1.pdf)]
- A Fast Unified Model for Parsing and Sentence Understanding [[arxiv](https://arxiv.org/pdf/1603.06021v2.pdf)]
- A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues [[arxiv](https://arxiv.org/pdf/1605.06069v3.pdf)]
- Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation [[arxiv](https://arxiv.org/pdf/1606.00776v2.pdf)]
- Sequence-to-Sequence Learning as Beam-Search Optimization [[arxiv](http://arxiv.org/pdf/1606.02960.pdf)] 
- Tables as Semi-structured Knowledge for Question Answering [[arxiv](http://ai2-website.s3.amazonaws.com/publications/FRETS-ACL-2016.pdf)]
- Ask Me Anything: Dynamic Memory Networks for Natural Language Processing [[arxiv](http://arxiv.org/pdf/1506.07285v5.pdf)]
- Dynamic Memory Networks for Visual and Textual Question Answering [[arxiv](https://arxiv.org/pdf/1603.01417v1.pdf)]
- http://homes.cs.washington.edu/~nasmith/papers/flanigan+dyer+smith+carbonell.naacl16.pdf [[arxiv](http://homes.cs.washington.edu/~nasmith/papers/flanigan+dyer+smith+carbonell.naacl16.pdf)]
- Iterative Alternating Neural Attention for Machine Reading [[arxiv](http://arxiv.org/pdf/1606.02245.pdf)]
- Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations [[arxiv](http://arxiv.org/pdf/1606.01305.pdf)]
- Vector-based Models of Semantic Composition [[arxiv](https://www.aclweb.org/anthology/P/P08/P08-1028.pdf)]
 - additive composition best
- Generating Natural Language Inference Chains [[arxiv](https://arxiv.org/pdf/1606.01404v1.pdf)]
- Learning to Compose Neural Networks for Question Answering [[arxiv](https://arxiv.org/pdf/1601.01705v4.pdf)]
- A Latent Variable Recurrent Neural Network for Discourse Relation Language Models [[arxiv](http://arxiv.org/pdf/1603.01913v2.pdf)]
- Data Recombination for Neural Semantic Parsing [[arxiv](https://arxiv.org/pdf/1606.03622v1.pdf)]
- Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data [[arxiv](https://arxiv.org/pdf/1606.03632v1.pdf)]
- Deep Reinforcement Learning with a Combinatorial Action Space for Predicting and Tracking Popular Discussion Threads [[arxiv](https://arxiv.org/pdf/1606.03667v1.pdf)]
- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets [[arxiv](https://arxiv.org/pdf/1606.03657v1.pdf)]
- A Diversity-Promoting Objective Function for Neural Conversation Models [[arxiv](https://arxiv.org/pdf/1510.03055v3.pdf)]
- Neural Associative Memory for Dual-Sequence Modeling [[arxiv](https://arxiv.org/pdf/1606.03864v2.pdf)]
- Key-Value Memory Networks for Directly Reading Documents [[arxiv](https://arxiv.org/pdf/1606.03126v1.pdf)]
- Simple Question Answering by Attentive Convolutional Neural Network [[arxiv](https://arxiv.org/pdf/1606.03391v1.pdf)]
- Neural Network-Based Abstract Generation for Opinions and Arguments [[arxiv](https://arxiv.org/pdf/1606.02785v1.pdf)]
- A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task [[arxiv](https://arxiv.org/pdf/1606.02858v1.pdf)]
- Generating Natural Questions About an Image [[arxiv](https://arxiv.org/pdf/1603.06059v3.pdf)]
- Continuously Learning Neural Dialogue Management [[arxiv](https://arxiv.org/pdf/1606.02689v1.pdf)]
- A Persona-Based Neural Conversation Model [[arxiv](https://arxiv.org/pdf/1603.06155v2.pdf)]
- Deep Reinforcement Learning for Dialogue Generation [[arxiv](https://arxiv.org/pdf/1606.01541v2.pdf)]
- A Decomposable Attention Model for Natural Language Inference [[arxiv](https://arxiv.org/abs/1606.01933)]
 - attention matrixs to decompose the problem into subproblems that can be solved separately
- CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases [[arxiv](https://arxiv.org/abs/1606.01994)]
- Memory-enhanced Decoder for Neural Machine Translation [[arxiv](https://arxiv.org/abs/1606.02003)]
- Incorporating Discrete Translation Lexicons into Neural Machine [[arxiv](https://arxiv.org/abs/1606.02006)]
- Can neural machine translation do simultaneous translation? [[arxiv](https://arxiv.org/abs/1606.02012)]
- Iterative Alternating Neural Attention for Machine Reading [[arxiv](https://arxiv.org/abs/1606.02245)]
- Language to Logical Form with Neural Attention [[arxiv](https://arxiv.org/abs/1601.01280)]
- Modeling Coverage for Neural Machine Translation [[arxiv](https://arxiv.org/abs/1601.04811)]
- Neural Summarization by Extracting Sentences and Words [[arxiv](https://arxiv.org/abs/1603.07252)]
- Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation [[arxiv](http://arxiv.org/pdf/1606.00776.pdf)]
- Generalizing and Hybridizing Count-based and Neural Language Models [[arxiv](http://arxiv.org/pdf/1606.00499.pdf)]


#### 2016-05
- A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues [[arxiv](https://arxiv.org/abs/1605.06069)]
- One-shot Learning with Memory-Augmented Neural Networks [[arxiv](https://arxiv.org/abs/1605.06065)]
- Residual Networks are Exponential Ensembles of Relatively Shallow Networks [[arxiv](https://arxiv.org/abs/1605.06431)]
- Dialog-based Language Learning [[arxiv](https://arxiv.org/abs/1604.06045)]
- Modelling Interaction of Sentence Pair with coupled-LSTMs [[arxiv](https://arxiv.org/abs/1605.05573)]
- Functional Hashing for Compressing Neural Networks [[arxiv](https://arxiv.org/abs/1605.06560)]
- Combining Recurrent and Convolutional Neural Networks for Relation Classification [[arxiv](https://arxiv.org/abs/1605.07333)]
- Learning End-to-End Goal-Oriented Dialog [[arxiv](https://arxiv.org/abs/1605.07683)]
- Variational Neural Machine Translation [[arxiv](https://arxiv.org/abs/1605.07869)]
- BattRAE: Bidimensional Attention-Based Recursive Autoencoders for Learning Bilingual Phrase Embeddings [[arxiv](https://arxiv.org/abs/1605.07874)]
- Encode, Review, and Decode: Reviewer Module for Caption Generation [[arxiv](https://arxiv.org/abs/1605.07912)]
- Automatic Extraction of Causal Relations from Natural Language Texts: A Comprehensive Survey [[arxiv](https://arxiv.org/abs/1605.07895)]
- A Convolutional Attention Network for Extreme Summarization of Source Code [[arxiv](https://arxiv.org/abs/1602.03001)]
- Learning language games through interaction
- Data recombination for neural semantic parsing.
- Inferring logical forms from denotations
- How much is 131 million dollars? putting numbers in perspective with compositional descriptions
- Learning to Generate with Memory [[arxiv](https://arxiv.org/abs/1602.07416)]
- Attention Correctness in Neural Image Captioning [[arxiv](https://arxiv.org/abs/1605.09553)]
- Contextual LSTM (CLSTM) models for Large scale NLP tasks [[arxiv](https://arxiv.org/abs/1602.06291)]
- Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations [[arxiv](http://scholar.google.com/scholar_url?url=http://arxiv.org/pdf/1605.07154&hl=en&sa=X&scisig=AAGBfm2Lak8dC2i-uqNOWf_1NddCevSRsw&nossl=1&oi=scholaralrt)]
- Generative Adversarial Text to Image Synthesis [[arxiv](http://scholar.google.com/scholar_url?url=http://arxiv.org/pdf/1605.05396&hl=en&sa=X&scisig=AAGBfm0q012JcYES6VnPaC7f1CA6iVu4Qw&nossl=1&oi=scholaralrt)]
- Query-Efficient Imitation Learning for End-to-End Autonomous Driving [[arxiv](http://scholar.google.co.kr/scholar_url?url=http://arxiv.org/pdf/1605.06450&hl=en&sa=X&scisig=AAGBfm3X_JBDyl61ASoWJEC5dnjFfVG22Q&nossl=1&oi=scholaralrt)]
- Hierarchical Memory Networks [[arxiv](http://scholar.google.com/scholar_url?url=http://arxiv.org/pdf/1605.07427&hl=en&sa=X&scisig=AAGBfm04HflMsgH5-Gm787iS-Oa_p4vKFA&nossl=1&oi=scholaralrt)]
- odelling Interaction of Sentence Pair with coupled-LSTMs [[arsiv](https://arxiv.org/abs/1605.05573)]
- Recurrent Neural Network for Text Classification with Multi-Task
  Learning [[arxiv](https://arxiv.org/abs/1605.05101)]
- Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling [[arxiv](https://arxiv.org/abs/1605.05110)]
- Rationale-Augmented Convolutional Neural Networks for Text
  Classification [[arxiv](https://arxiv.org/abs/1605.04469)]
- Reducing the Model Order of Deep Neural Networks Using Information
  Theory [[arxiv](https://arxiv.org/abs/1605.04859)]
- Compressing Word Embeddings [[arxiv](https://arxiv.org/abs/1511.06397)]
- Joint Event Extraction via Recurrent Neural Networks [[paper](http://scholar.google.co.kr/scholar_url?url=http://www.cs.nyu.edu/~thien/pubs/jointEE.pdf&hl=en&sa=X&scisig=AAGBfm1wUfB2tSO_zEvLwf0tPHJ1igLihw&nossl=1&oi=scholaralrt)]
- Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model [[paper](http://scholar.google.co.kr/scholar_url?url=http://arxiv.org/pdf/1605.03835&hl=en&sa=X&scisig=AAGBfm0D7ymB1zzlOBj8mGIrF47mc-7j0g&nossl=1&oi=scholaralrt)]
- Joint Extraction of Events and Entities within a Document Context [[paper](http://scholar.google.co.kr/scholar_url?url=http://www.cs.cmu.edu/~bishan/papers/joint_event_naacl16.pdf&hl=en&sa=X&scisig=AAGBfm12l_rfjffmHbRAg6VW26V4nSvhtQ&nossl=1&oi=scholaralrt)]
- Natural Language Semantics and Computability [[arxiv](https://arxiv.org/abs/1605.04122)]
- Transfer Hashing with Privileged Information [[arxiv](https://arxiv.org/abs/1605.04034)]
- Natural Language Inference by Tree-Based Convolution and Heuristic
  Matching [[arxiv](https://arxiv.org/abs/1512.08422)]
- Generating Sentences from a Continuous Space [[arxiv](https://arxiv.org/abs/1511.06349)]
- Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic
  Representation Learning [[arxiv](https://arxiv.org/abs/1605.03832)]
- Learning the Curriculum with Bayesian Optimization for Task-Specific
  Word Representation Learning [[arxiv](https://arxiv.org/abs/1605.03852)]
- Vocabulary Manipulation for Neural Machine Translation [[arxiv](https://arxiv.org/abs/1605.03209)]
- Tweet2Vec: Character-Based Distributed Representations for Social Media [[arxiv](https://arxiv.org/abs/1605.03481)]
- Chained Predictions Using Convolutional Neural Networks [[arxiv](http://arxiv.org/abs/1605.02346)]
- Boltzmann meets Nash: Energy-efficient routing in optical networks under uncertainty [[arxiv](https://arxiv.org/abs/1605.01451)]
- Modeling Rich Contexts for Sentiment Classification with LSTM [[arxiv](https://arxiv.org/abs/1605.01478)]
- Incorporating Selectional Preferences in Multi-hop Relation Extraction [[naacl16](http://rajarshd.github.io/papers/naaclakbc2016.pdf)]
- Word Ordering Without Syntax [[arxiv](http://arxiv.org/pdf/1604.08633.pdf)]
- Compositional Sentence Representation from Character within Large Context Text [[arxiv](https://arxiv.org/abs/1605.00482)]
- Abstractive Sentence Summarization with Attentive Recurrent Neural Networks [[arxiv](http://harvardnlp.github.io/papers/naacl16_summary.pdf)]
- Mixed Incremental Cross-Entropy REINFORCE ICLR 2016 [[github](https://github.com/facebookresearch/MIXER)]
- A library for probabilistic modeling, inference, and criticism. Deep generative models, variational inference. Runs on TensorFlow. [[github](https://github.com/blei-lab/edward)]

#### 2016-04
- Dialog-based Language Learning [[arxiv](https://arxiv.org/abs/1604.06045)]
- Towards Conceptual Compression [[arxiv](https://arxiv.org/abs/1604.08772)]]
- Teaching natural language to computers [[arxiv](https://arxiv.org/abs/1604.08781)]
- Pointing the Unknown Words
- Attend, Infer, Repeat Fast Scene Understanding with Generative Models
- How NOT To Evaluate Your Dialogue System An Empirical Study of
- Revisiting Semi-Supervised Learning with Graph Embeddings
- Neural Summarization by Extracting Sentences and Words
- Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints
- LSTM-BASED DEEP LEARNING MODELS FOR NONFACTOID
- Generating Visual Explanations
- A Compositional Approach to Language Modeling [[arxiv](http://arxiv.org/abs/1604.00100)]
- AttSum: Joint Learning of Focusing and Summarization with Neural Attention [[arxiv](http://arxiv.org/abs/1604.00125)]
- Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems [[arxiv](http://arxiv.org/abs/1511.06931)]
- The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations [[arxiv](http://arxiv.org/abs/1511.02301)]
- Building Machines That Learn and Think Like People [[arxiv](Building Machines That Learn and Think Like People)]
- A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories [[arxiv](https://arxiv.org/abs/1604.01696)]
- Revisiting Summarization Evaluation for Scientific Articles [[arxiv](http://arxiv.org/abs/1604.00400)]
- Reasoning About Pragmatics with Neural Listeners and Speakers [[arxiv](http://arxiv.org/abs/1604.00562)]
- Character-Level Question Answering with Attention [[arxiv](http://arxiv.org/abs/1604.00727)]
- Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks [[arxiv](http://arxiv.org/abs/1604.00734)]
- Top-down Tree Long Short-Term Memory Networks [[arxiv](http://arxiv.org/abs/1511.00060)]
- Recurrent Neural Network Grammars [[arxiv](http://arxiv.org/abs/1602.07776)]


#### 2016-03
- Pointing the Unknown Words [[arxiv](http://arxiv.org/abs/1603.08148)]
- Neural Programmer: Inducing Latent Programs with Gradient Descent [[arxiv](http://scholar.google.com/scholar_url?url=https://research.google.com/pubs/archive/44927.pdf&hl=en&sa=X&scisig=AAGBfm2VedkF99f2i9IB7m_Ki5ELxJ-SCQ&nossl=1&oi=scholaralrt)]
- Adversarial Autoencoders []
- Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition
- Net2Net: Accelerating Learning via Knowledge Transfer
- Neural Programmer: Inducing Latent Programs with Gradient Descent
- A Neural Conversational Model
- Document embedding with paragraph vectors
- Neural Language Correction with Character-Based Attention [[arxiv](http://arxiv.org/abs/1603.09727)]
- Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks [[arxiv](http://arxiv.org/abs/1604.01178)]
- Automatic Annotation of Structured Facts in Images
- Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves [[arxiv](https://arxiv.org/abs/1604.02038)]
- Building Machines That Learn and Think Like People [[arxiv](http://arxiv.org/pdf/1604.00289.pdf)]
- LARGER-CONTEXT LANGUAGE MODELLING WITH RECURRENT NEURAL NETWORK [[arxiv](http://arxiv.org/pdf/1511.03729v2.pdf)]
- A Diversity-Promoting Objective Function for Neural Conversation Model [[arxiv](http://arxiv.org/pdf/1510.03055v2.pdf)]
- TOWARDS PRINCIPLED UNSUPERVISED LEARNING [[arxiv](http://arxiv.org/pdf/1511.06440v2.pdf)]
- Sentence-Level Grammatical Error Identification as Sequence-to-Sequence
  Correction [[arxiv](https://arxiv.org/abs/1604.04677)]
- Hierarchical Attention Networks for Document Classification [[arxiv](http://scholar.google.co.kr/scholar_url?url=http://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf&hl=ko&sa=X&scisig=AAGBfm3Yksk0QUL3dBcaokFPC3DOF8CFvg&nossl=1&oi=scholaralrt)]
- Visual Storytelling [[arxiv](https://arxiv.org/abs/1604.03968)]
- Using Sentence-Level LSTM Language Models for Script Inference [[arxiv](https://arxiv.org/abs/1604.02993)]
- ABCNN: Attention-Based Convolutional Neural Network for Modeling
  Sentence Pairs [[arxiv](https://arxiv.org/abs/1512.05193)]
- Character-Level Question Answering with Attention [[arxiv](https://arxiv.org/abs/1604.00727)]
- Abstractive Text Summarization Using Sequence-to-Sequence RNNs and
  Beyond [[arxiv](https://arxiv.org/abs/1602.06023)]
- Sentence Compression by Deletion with LSTMs [[link](http://research.google.com/pubs/pub43852.html)]
- A Simple Way to Initialize Recurrent Networks of Rectified Linear Units [[arxiv](http://arxiv.org/abs/1504.00941)]
- DenseCap: Fully Convolutional Localization Networks for Dense Captioning [[arxiv](http://cs.stanford.edu/people/karpathy//densecap.pdf)]
- TRAINING NEURAL NETWORKS ON PARTITIONED TRAINING DATA [[paper]()]
- Nonextensive information theoretical machine [[arxiv](https://arxiv.org/abs/1604.06153)]
- Training Deep Nets with Sublinear Memory Cost [[arxiv](https://arxiv.org/abs/1604.06174)]
- What we write about when we write about causality: Features of causal statements across large-scale social discourse [[arxiv](https://arxiv.org/abs/1604.05781)]
- Dialog-based Language Learning [[arxiv](https://arxiv.org/abs/1604.06045)]
- Question Answering via Integer Programming over Semi-Structured
  Knowledge [[arxiv](https://arxiv.org/abs/1604.06076)]
- Dialog-based Language Learning [[arxiv](http://arxiv.org/pdf/1604.06045.pdf)]
- Bridging LSTM Architecture and the Neural Dynamics during Reading [[arxiv](https://arxiv.org/abs/1604.06635)]
- Neural Generative Question Answering [[arxiv](https://arxiv.org/abs/1512.01337)]
- Recurrent Memory Networks for Language Modeling [[arxiv](https://arxiv.org/abs/1601.01272)]
- Colorful Image Colorization [[paper](http://arxiv.org/abs/1603.08511)]  [[code](https://github.com/richzhang/colorization)] [[note](/notes/Colorful-Image-Colorization.md)]
